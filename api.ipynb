{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-06 19:27:22--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "Resolving huggingface.co (huggingface.co)... 13.35.191.58, 13.35.191.66, 13.35.191.115, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.35.191.58|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/97d9becd5a364323c7959cc82e7506d6eb26c025623320b844e45e517e3dfe76?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_1.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1691588779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MTU4ODc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3Lzk3ZDliZWNkNWEzNjQzMjNjNzk1OWNjODJlNzUwNmQ2ZWIyNmMwMjU2MjMzMjBiODQ0ZTQ1ZTUxN2UzZGZlNzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=GPGBcDBttFTwYtTeVRTa4Y5BOeBVvGnTWUYaGbCJ4tReWah9ww4-20artMCq7g5H2-kuhaItaBQc2ffrwjjfykce5WtIgXwWvORohQfz1g8ouW0B77bzEX9fA-UCpVI2Cfv-OVXScAJy9L88fLvK0H5ozqnjg7S78GZ%7Ebd%7ENuHQkLqz8p6XMp90NWeR5YboOB2evFm2B0KgUyJVXUBGcq8oaeRpZSgb7tJagHMO29f1FdDavjIrkRATjxGwzQhwTnKdduQd3TqurMMJDAbe0XwVoUgZ7TzX%7EE73otsbUtc7lFux-w8vBkDw6VtVl5h9jHdvstQLXq0v-8rU02M5knw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-08-06 19:27:23--  https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/97d9becd5a364323c7959cc82e7506d6eb26c025623320b844e45e517e3dfe76?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_1.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1691588779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MTU4ODc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3Lzk3ZDliZWNkNWEzNjQzMjNjNzk1OWNjODJlNzUwNmQ2ZWIyNmMwMjU2MjMzMjBiODQ0ZTQ1ZTUxN2UzZGZlNzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=GPGBcDBttFTwYtTeVRTa4Y5BOeBVvGnTWUYaGbCJ4tReWah9ww4-20artMCq7g5H2-kuhaItaBQc2ffrwjjfykce5WtIgXwWvORohQfz1g8ouW0B77bzEX9fA-UCpVI2Cfv-OVXScAJy9L88fLvK0H5ozqnjg7S78GZ%7Ebd%7ENuHQkLqz8p6XMp90NWeR5YboOB2evFm2B0KgUyJVXUBGcq8oaeRpZSgb7tJagHMO29f1FdDavjIrkRATjxGwzQhwTnKdduQd3TqurMMJDAbe0XwVoUgZ7TzX%7EE73otsbUtc7lFux-w8vBkDw6VtVl5h9jHdvstQLXq0v-8rU02M5knw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.164.202.109, 18.164.202.78, 18.164.202.9, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.164.202.109|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9763701888 (9.1G) [application/octet-stream]\n",
      "Saving to: ‘llama-2-13b-chat.ggmlv3.q5_1.bin’\n",
      "\n",
      "      llama-2-13b-c   0%[                    ]   1.34M  1.21MB/s               ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q5_1.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7349.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama.cpp: loading model from ../llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7349.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n"
     ]
    }
   ],
   "source": [
    "from colabcode import ColabCode\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from custom_llm_model import CustomLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7349.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n"
     ]
    }
   ],
   "source": [
    "llm_model = CustomLLM()\n",
    "app = FastAPI()\n",
    "cc = ColabCode(port=12000, code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInput(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "\n",
    "@app.post(\"/\")\n",
    "async def read_root(user_input: UserInput):\n",
    "    output = llm_model(user_input.prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2023-08-06T19:23:32+0530 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/home/gagan/.config/ngrok/ngrok.yml legacy_path=/home/gagan/.ngrok2/ngrok.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [14438]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://672f-122-161-90-133.ngrok.io\" -> \"http://localhost:12000\"\n",
      "INFO:     2401:4900:1c68:21ad:c924:8b8e:418f:52a3:0 - \"GET / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     2401:4900:1c68:21ad:c924:8b8e:418f:52a3:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     122.161.90.133:0 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [14438]\n"
     ]
    }
   ],
   "source": [
    "cc.run_app(app=app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
